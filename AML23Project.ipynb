{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdd9bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, accuracy_score, silhouette_score, davies_bouldin_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from enum import Enum\n",
    "from kneed import KneeLocator\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cace29",
   "metadata": {},
   "source": [
    "###### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b756c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data = pd.read_csv('datasets/Spotify_Dataset_V3.csv', delimiter=';')\n",
    "song_data.info()\n",
    "song_data.describe()\n",
    "spotify_song_data = song_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf2a118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换“Date”列的数据类型为 'datetime'，确保正确解析日期格式\n",
    "spotify_song_data['Date'] = pd.to_datetime(spotify_song_data['Date'], format='%d/%m/%Y')\n",
    "\n",
    "# 创建一个新列“Data_Month”，只包含年份和月份\n",
    "spotify_song_data['Data_Month'] = spotify_song_data['Date'].dt.to_period('M')\n",
    "\n",
    "# 显示更新后的数据格式\n",
    "spotify_song_data.head()\n",
    "\n",
    "# 计算每首歌每月的平均分数\n",
    "average_monthly_points = spotify_song_data.groupby(['id', 'Data_Month'])['Points (Total)'].mean().reset_index()\n",
    "\n",
    "# 重命名列以反映这是平均值\n",
    "average_monthly_points.rename(columns={'Points (Total)': 'Average_Points'}, inplace=True)\n",
    "\n",
    "# Merge this new data with the original data to maintain all information\n",
    "monthly_data = pd.merge(spotify_song_data, average_monthly_points, on=['id', 'Data_Month'], how='left')\n",
    "\n",
    "# Dropping daily points and other unnecessary columns\n",
    "columns_to_drop = ['Points (Total)', 'Points (Ind for each Artist/Nat)', 'Date', 'Rank']\n",
    "monthly_data.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "# Dropping duplicates as we now have monthly data\n",
    "monthly_data.drop_duplicates(inplace=True)\n",
    "\n",
    "monthly_data.head()\n",
    "\n",
    "\n",
    "# 根据 'Data_Month' 和 'Average_Points' 对数据进行排序\n",
    "monthly_data.sort_values(by=['Data_Month', 'Average_Points'], ascending=[True, False], inplace=True)\n",
    "\n",
    "# 由于每首歌可能有多个艺术家，我们需要确保这些信息被保留\n",
    "# 我们将创建一个函数来聚合艺术家信息，并应用这个函数\n",
    "\n",
    "def aggregate_artists(series):\n",
    "    return \", \".join(series.unique())  # 保留唯一的艺术家名字，避免重复\n",
    "\n",
    "# 对 'Artists' 列应用此函数\n",
    "monthly_data_grouped = monthly_data.groupby(['Title', 'Data_Month', 'id', 'Song URL', 'Average_Points'], as_index=False)\n",
    "monthly_data_final = monthly_data_grouped.agg({\n",
    "    'Artists': aggregate_artists,\n",
    "    'Danceability': 'first',\n",
    "    'Energy': 'first',\n",
    "    'Loudness': 'first',\n",
    "    'Speechiness': 'first',\n",
    "    'Acousticness': 'first',\n",
    "    'Instrumentalness': 'first',\n",
    "    'Valence': 'first',\n",
    "    'Continent': 'first',\n",
    "    'Nationality': 'first'  # 如果有多个国籍，此处可能需要额外的逻辑\n",
    "})\n",
    "\n",
    "monthly_data_final.head()  # 展示处理后的数据集的前几行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300bffb2",
   "metadata": {},
   "source": [
    "#### What drives cross-regional popularity of music; is it the artist, or something about the song?\n",
    "1. Identify the correlation between musical attribution and point\n",
    "2. FInd out the correlation between artists, nationality and point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e12f13c",
   "metadata": {},
   "source": [
    "Firstly, identify the correlation between musical attribution and point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6005f7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr_matrix = spotify_song_data_reduced.corr()\n",
    "\n",
    "# remove the first row and last column\n",
    "corr_matrix = corr_matrix.iloc[1:, :-1]\n",
    "\n",
    "# Generate a mask\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k = 1)\n",
    "\n",
    "# Set matplotlib figure\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Generate colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap\n",
    "sns.heatmap(corr_matrix, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n",
    "\n",
    "# Title and display the plot\n",
    "plt.title(\"Numeric Feature Correlation Heatmap\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f9b23d",
   "metadata": {},
   "source": [
    "We can clearly see that there are not any correlation between musical attribution and Point almost\n",
    "But we can still concentrate on whether exists the correlation between musical attribution and different nationalities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99849c5c",
   "metadata": {},
   "source": [
    "Due to the data of nationalities are too many, so we transfer them to continent, identify the correlaiton of continent and attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b21194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating means of the musical features in each continent\n",
    "musical_features = ['Danceability', 'Energy', 'Loudness', 'Speechiness', \n",
    "                    'Acousticness', 'Instrumentalness', 'Valence']\n",
    "mean_values_per_continent = spotify_song_data.groupby('Continent')[musical_features].mean()\n",
    "\n",
    "# Displaying the means\n",
    "mean_values_per_continent.sort_values(by='Danceability', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac99349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set matplotlib figure\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Create subplots for each musical feature\n",
    "for i, feature in enumerate(musical_features, 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    sns.boxplot(x='Continent', y=feature, data=spotify_song_data)\n",
    "    plt.title(f'Boxplot of {feature} by Continent')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Display the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c35828",
   "metadata": {},
   "source": [
    "Danceability：Latin-America的歌曲在可舞动性上的中位数最高\n",
    "\n",
    "Energy：不同大陆在能量特质上的分布差异较大，其中“Unknown”大陆的中位数最高\n",
    "\n",
    "Loudness：大多数大陆在响度上的分布相近，但“Anglo-America”稍微较低\n",
    "\n",
    "Speechiness：Latin-America的歌曲在说唱性上的分布最高\n",
    "\n",
    "Acousticness：Latin-America的歌曲在声学性上的中位数也较高\n",
    "\n",
    "Instrumentalness：“Unknown”大陆在乐器性上的中位数最高\n",
    "\n",
    "Valence：Latin-America在正向情感度上的分布也较高"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdceae7",
   "metadata": {},
   "source": [
    "Next, we find out if there are corellation between artist and point\n",
    "\n",
    "Since the 'Artists' field seems to contain multiple artists in some records, we will use 'Artist (Ind.)' for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa9412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the average points for each artist\n",
    "average_points_per_artist = spotify_song_data.groupby('Artist (Ind.)')['Points (Total)'].mean()\n",
    "\n",
    "# Converting the series to a dataframe and resetting the index\n",
    "average_points_df = average_points_per_artist.reset_index()\n",
    "\n",
    "# Renaming the columns for clarity\n",
    "average_points_df.rename(columns={'Points (Total)': 'Average Points'}, inplace=True)\n",
    "\n",
    "# Displaying the first few rows of the dataframe\n",
    "average_points_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ccd582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the top artists based on average points for visualization\n",
    "top_artists = average_points_df.nlargest(20, 'Average Points')\n",
    "\n",
    "# Creating a bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(top_artists['Artist (Ind.)'], top_artists['Average Points'], color='skyblue')\n",
    "plt.xlabel('Average Points')\n",
    "plt.title('Top 20 Artists with Highest Average Points')\n",
    "plt.gca().invert_yaxis()  # To have the highest on top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089ee45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by artist and calculating the total points for each artist\n",
    "artist_total_points = spotify_song_data.groupby('Artist (Ind.)')['Points (Ind for each Artist/Nat)'].sum()\n",
    "\n",
    "# Sorting the artists by total points and selecting the top 20\n",
    "top_artists = artist_total_points.sort_values(ascending=False).head(20)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=top_artists.values, y=top_artists.index, palette='viridis')\n",
    "plt.xlabel('Total Points (Ind for each Artist/Nat)', fontsize=12)\n",
    "plt.ylabel('Artist', fontsize=12)\n",
    "plt.title('Top 20 Artists Based on Total Points (Ind for each Artist/Nat)', fontsize=15)\n",
    "plt.grid(axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5f7274",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculating average Points per continent\n",
    "avg_points_per_continent = spotify_song_data.groupby('Continent')['Points (Ind for each Artist/Nat)'].mean().sort_values(ascending=False)\n",
    "\n",
    "# Plotting boxplot for Points distribution per continent\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=spotify_song_data, x='Continent', y='Points (Ind for each Artist/Nat)', palette='viridis')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Continent', fontsize=12)\n",
    "plt.ylabel('Points (Ind for each Artist/Nat)', fontsize=12)\n",
    "plt.title('Distribution of Points (Ind for each Artist/Nat) per Continent', fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Displaying average Points per continent\n",
    "avg_points_per_continent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1082d4f2",
   "metadata": {},
   "source": [
    "we can see a huge difference, so it shows artist is an important factor for cross-reginal popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c2eb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA Test\n",
    "f_stat, p_value = stats.f_oneway(*(spotify_song_data['Points (Ind for each Artist/Nat)'][spotify_song_data['Continent'] == continent] for continent in spotify_song_data['Continent'].unique()))\n",
    "\n",
    "(f_stat, p_value)\n",
    "\n",
    "# Since the p-value is much smaller than the common significance level\n",
    "\n",
    "# This shows that the continent may indeed be related to the Points of the song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cbcb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Besides, nationality would like to be a factor\n",
    "artist_count_per_nationality = spotify_song_data['Nationality'].value_counts()\n",
    "\n",
    "# Selecting the top 10 nationalities that have the most artists\n",
    "top_nationalities_by_count = artist_count_per_nationality.head(10).index\n",
    "\n",
    "# Calculating the average points for these top nationalities\n",
    "avg_points_top_nationalities = spotify_song_data[spotify_song_data['Nationality'].isin(top_nationalities_by_count)].groupby('Nationality')['Points (Ind for each Artist/Nat)'].mean().sort_values(ascending=False)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=avg_points_top_nationalities.index, y=avg_points_top_nationalities.values, palette='viridis')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Nationality', fontsize=12)\n",
    "plt.ylabel('Average Points (Ind for each Artist/Nat)', fontsize=12)\n",
    "plt.title('Average Points for Top 10 Nationalities (by Artist Count)', fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Displaying the data\n",
    "avg_points_top_nationalities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fffa18b",
   "metadata": {},
   "source": [
    "Can we figure out which artists or genres are going to be popular in 2024 given the historic data from 2017?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a4b706",
   "metadata": {},
   "source": [
    "###### Data preprogressing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80a318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new column for each artists' average score\n",
    "\n",
    "# Step 1: Calculate the average points for each artist\n",
    "artist_avg_points = spotify_song_data.groupby('Artist (Ind.)')['Points (Total)'].mean()\n",
    "\n",
    "# Step 2: Function that returns the average points of artists\n",
    "def get_artist_average_points(artists):\n",
    "    # For multiple artists, we calculate the mean of their average points\n",
    "    artist_list = artists.split(\", \")  # Assuming artists are separated by \", \"\n",
    "    avg_points_list = [artist_avg_points.get(artist, 0) for artist in artist_list]\n",
    "    return sum(avg_points_list) / len(avg_points_list) if avg_points_list else 0\n",
    "\n",
    "# Step 3: Apply the function to the 'Artists' column\n",
    "spotify_song_data['Artist_Average_Points'] = spotify_song_data['Artists'].apply(get_artist_average_points)\n",
    "\n",
    "# Show the first few rows of the dataframe to verify the new column\n",
    "spotify_song_data[['Artists', 'Artist_Average_Points']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cb80d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_one_hot = pd.get_dummies(spotify_song_data, columns=['Continent'])\n",
    "\n",
    "# Applying one-hot encoding to the 'Continent' column and adding it to the original dataframe\n",
    "spotify_song_data = pd.concat([spotify_song_data, pd.get_dummies(spotify_song_data['Continent'], prefix='Continent')], axis=1)\n",
    "\n",
    "spotify_song_data.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9debe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Grouping the data by artist and calculating the total points\n",
    "artist_total_points = spotify_song_data.groupby('Artist (Ind.)')['Points (Ind for each Artist/Nat)'].sum()\n",
    "\n",
    "# Step 2: Creating a new column in the original dataframe that maps the total points for each artist\n",
    "spotify_song_data['Artist_Total_Points'] = spotify_song_data['Artist (Ind.)'].map(artist_total_points)\n",
    "\n",
    "spotify_song_data[['Artist (Ind.)', 'Artist_Total_Points']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf3515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting year and month\n",
    "spotify_song_data['Year'] = pd.to_datetime(spotify_song_data['Date']).dt.year\n",
    "spotify_song_data['Month'] = pd.to_datetime(spotify_song_data['Date']).dt.month\n",
    "\n",
    "spotify_song_data[['Date', 'Year', 'Month']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abf6583",
   "metadata": {},
   "source": [
    "###### Model: logistic regression, random forest, naive bayes(baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497dad72",
   "metadata": {},
   "source": [
    "我们目前处理了三个因素，Artist_Average_Points, Artist_Total_Points, different continent. \n",
    "现在我对2017年1月-2017年12月的数据作为训练集进行学习，并根据2023年1月到2023年5月的数据作为对照集测试，\n",
    "model用logistic regression, random forest, naive bayes，把Point前25%的歌曲表示为流行，预测歌曲是否流行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faedeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organized Code for the Entire Process\n",
    "\n",
    "# 1. Data Preprocessing\n",
    "\n",
    "# Extracting year and month from the 'Date' column\n",
    "spotify_song_data['Year'] = pd.to_datetime(spotify_song_data['Date']).dt.year\n",
    "spotify_song_data['Month'] = pd.to_datetime(spotify_song_data['Date']).dt.month\n",
    "\n",
    "# Defining the target variable based on the top 25% of points\n",
    "threshold = spotify_song_data['Points (Total)'].quantile(0.75)\n",
    "spotify_song_data['Popular'] = (spotify_song_data['Points (Total)'] >= threshold).astype(int)\n",
    "\n",
    "# 2. Feature Selection and Dataset Splitting\n",
    "\n",
    "# Selecting features and target\n",
    "continent_features = [col for col in spotify_song_data.columns if col.startswith('Continent_')]\n",
    "features = ['Artist_Average_Points', 'Artist_Total_Points'] + continent_features\n",
    "target = 'Popular'\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "train_data = spotify_song_data[(spotify_song_data['Year'] >= 2017)]\n",
    "#                                & (spotify_song_data['Year'] <= 2022)]\n",
    "test_data = spotify_song_data[(spotify_song_data['Year'] == 2023)]\n",
    "\n",
    "X_train = train_data[features]\n",
    "y_train = train_data[target]\n",
    "X_test = test_data[features]\n",
    "y_test = test_data[target]\n",
    "\n",
    "# Normalizing the feature data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 3. Model Training\n",
    "\n",
    "# declare model:\n",
    "# Training the logistic regression model\n",
    "lr_model = LogisticRegression(random_state=0, class_weight='balanced', verbose=1)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# random forest model training\n",
    "rf_model = RandomForestClassifier(random_state=0, class_weight='balanced', verbose=1)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Naive Bayes model training\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 4. Model Evaluation\n",
    "print(f\"{' Model Evaluation Results ':-^60}\\n\")\n",
    "for model in [lr_model, rf_model, nb_model]:\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    classification_rep = classification_report(y_test, y_pred)\n",
    "    print(f'Model: {model.__class__.__name__}')\n",
    "    print(f'Accuracy: {accuracy:.2f}')\n",
    "    print(f'Classification Report:\\n {classification_rep}\\n')\n",
    "print(\"-\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f16a863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation_report(accuracy, classification_rep):\n",
    "    \"\"\"\n",
    "    Print the evaluation report in a formatted manner.\n",
    "\n",
    "    Parameters:\n",
    "    accuracy (float): The accuracy score of the model.\n",
    "    classification_rep (str): The classification report as a string.\n",
    "    \"\"\"\n",
    "    print(f\"{' Model Evaluation Results ':-^60}\\n\")\n",
    "    print(f\"Overall Accuracy: {accuracy:.2%}\\n\")\n",
    "    print(\"Detailed Classification Report:\")\n",
    "    print(classification_rep)\n",
    "    print(\"-\"*60)\n",
    "\n",
    "# 4. Model Evaluation\n",
    "\n",
    "# Evaluating the model on the test set\n",
    "y_pred = lr_model.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "# Printing the evaluation report using the new function\n",
    "print_evaluation_report(accuracy, classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7286ee",
   "metadata": {},
   "source": [
    "## Analyse regional result/song data\n",
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c85b35f",
   "metadata": {},
   "source": [
    "Regional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c459710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data_region = song_data.copy()\n",
    "# print the rows where the Date ends with '06/2023'\n",
    "\n",
    "# remove the song with same id, same date\n",
    "song_data_region = song_data_region.drop_duplicates(subset=['id', 'Date'], keep='first')\n",
    "feature_columns = ['Danceability', 'Energy', 'Loudness', 'Speechiness', 'Acousticness', 'Instrumentalness', 'Valence', 'Continent', 'Date', 'id']\n",
    "target_column = ['Points (Total)']\n",
    "song_data_region = song_data_region[feature_columns + target_column]\n",
    "\n",
    "# remove duplicates\n",
    "song_data_region = song_data_region.drop_duplicates()\n",
    "song_data_region['Year'] = song_data_region['Date'].str.split('/').str[2].astype(int)\n",
    "song_data_region['Month'] = song_data_region['Date'].str.split('/').str[1].astype(int)\n",
    "song_data_region['Day'] = song_data_region['Date'].str.split('/').str[0].astype(int)\n",
    "\n",
    "# drop date\n",
    "song_data_region = song_data_region.drop(columns=['Date'])\n",
    "\n",
    "# drop the row that continent is unknown\n",
    "song_data_region = song_data_region[song_data_region['Continent'] != 'Unknown']\n",
    "\n",
    "song_data_region.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e98f7f1",
   "metadata": {},
   "source": [
    "Pure Song data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af23fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data_only = song_data.copy()\n",
    "\n",
    "song_data_only = song_data_only.drop_duplicates(subset=['id'], keep='first')\n",
    "desired_columns = ['Danceability', 'Energy', 'Loudness', 'Speechiness', 'Acousticness', 'Instrumentalness', 'Valence', 'Continent']\n",
    "song_data_only = song_data_only[desired_columns]\n",
    "song_data_only.reset_index(drop=True, inplace=True)\n",
    "song_data_only.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b889833",
   "metadata": {},
   "source": [
    "#### Visualize the result (region)\n",
    "Typically balanced despite of some variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eb4182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time series visualization\n",
    "# plot the total number of songs by year and month\n",
    "# group by year and month\n",
    "line_graph_sum = song_data_region.groupby(['Year', 'Month']).size().reset_index(name='Number of Songs')\n",
    "\n",
    "# combine year and month\n",
    "line_graph_sum['Year-Month'] = line_graph_sum['Year'].astype(str) + '-' + line_graph_sum['Month'].astype(str)\n",
    "line_graph_data = line_graph_sum.drop(columns=['Year', 'Month'])\n",
    "line_graph_data.tail(10)\n",
    "# plot the line graph\n",
    "plt.figure(figsize=(20, 10))\n",
    "x = line_graph_data['Year-Month']\n",
    "y = line_graph_data['Number of Songs']\n",
    "plt.plot(x, y)\n",
    "plt.xticks(x[::5])\n",
    "plt.xlabel('Year-Month')\n",
    "plt.ylabel('Number of Songs')\n",
    "plt.title('Total Number of Songs by Year and Month')\n",
    "plt.show()\n",
    "line_graph_data.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89da5bab",
   "metadata": {},
   "source": [
    "### Analyse points of different regions over time\n",
    "In visualisation stage, we can not see any relations between the popularity of music in one continent and the popularity of music in other continents as we lack of information about that song in other continent For the time trend in terms of daily points, we can see that the average points across each day is quite stable, we can not make a conclusion that the popularity of music will increase or decrease in specific time period\n",
    "\n",
    "### Analyse the user's streams over time\n",
    "In the graph and data, we extract top 30% of the average score in that month. We assume that if the average score of the songs is high, it means that the user to spend longer time to listen to the user. We can see that there is lower average score in Month 1,2,9,12. So the average song quality maynot be good in these months. This could be an indicator that the user may not spend longer time to listen to the user. In 3, 6, 7, 10, 11, the average score is higher and that could increase the opportunity for the user to spend longer time to listen to the music. Publishing the music in these months could be a good choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e975a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mode(Enum):\n",
    "    MEAN: str = 'Average Points'\n",
    "    MEDIAN: str = 'Median Points'\n",
    "    SUM: str = 'Total Points'\n",
    "\n",
    "class GraphType(Enum):\n",
    "    SUBPLOTS: str = 'Subplots'\n",
    "    ONE_PLOT: str = 'One Plot'\n",
    "\n",
    "# grab top n average points of the songs in each continent in each year and month\n",
    "def top_n_song_points(data: pd.DataFrame, n: int = 10):\n",
    "    region_points = song_data_region.groupby(['Year', 'Month', 'Continent', 'id'])['Points (Total)'].max()\n",
    "    region_points_df = region_points.reset_index(name='Max Points')\n",
    "    top_n_songs_points = region_points_df.sort_values(by=['Year', 'Month', 'Continent', 'Max Points'], ascending=False)\n",
    "    top_n_songs_points = top_n_songs_points.groupby(['Year', 'Month', 'Continent']).head(n)\n",
    "    top_n_songs_points = top_n_songs_points.reset_index(drop=True)\n",
    "    return top_n_songs_points\n",
    "\n",
    "def line_graph_for_time_series(data: pd.DataFrame, mode = Mode.MEAN, outlier_percent: float = 0.25, xticks_interval: int = 5, day: bool = False, date_point_quantile: float = 0.0):\n",
    "    group_by_columns = ['Year', 'Month'] if not day else ['Year', 'Month', 'Day']\n",
    "    merged_time_column = 'Year-Month' if not day else 'Year-Month-Day'\n",
    "    # create a data with year, month and the max points of the song in each continent in each year and month\n",
    "    line_graph_points = data.groupby(group_by_columns + ['id'])['Points (Total)'].max()\n",
    "    line_graph_points = line_graph_points.reset_index(name='Max Points')\n",
    "    # drop last outlier_percent of the points in each year and month to remove outliers\n",
    "    line_graph_points = line_graph_points.sort_values(by=group_by_columns + ['Max Points'], ascending=False)\n",
    "    point_threshold = line_graph_points.groupby(group_by_columns)['Max Points'].quantile(outlier_percent)\n",
    "    line_graph_points = line_graph_points.merge(point_threshold, on=group_by_columns, suffixes=('', '_threshold'))\n",
    "    line_graph_points = line_graph_points[line_graph_points['Max Points'] >= line_graph_points['Max Points_threshold']]\n",
    "    line_graph_points = line_graph_points.drop(columns=['Max Points_threshold'])\n",
    "    line_graph_points = line_graph_points.reset_index(drop=True)\n",
    "    line_graph_points = line_graph_points.groupby(group_by_columns)['Max Points']\n",
    "    \n",
    "    if mode == Mode.MEAN:\n",
    "        line_graph_points = line_graph_points.mean()\n",
    "    elif mode == Mode.MEDIAN:\n",
    "        line_graph_points = line_graph_points.median()\n",
    "    elif mode == Mode.SUM:\n",
    "        line_graph_points = line_graph_points.sum()\n",
    "    \n",
    "    line_graph_points = line_graph_points.reset_index(name= mode.value)\n",
    "    if not day:\n",
    "        line_graph_points['Year-Month'] = line_graph_points['Year'].astype(str) + '-' + line_graph_points['Month'].astype(str)\n",
    "    else:\n",
    "        line_graph_points['Year-Month-Day'] = line_graph_points['Year'].astype(str) + '-' + line_graph_points['Month'].astype(str) + '-' + line_graph_points['Day'].astype(str)\n",
    "    line_graph_points = line_graph_points.drop(columns=group_by_columns)\n",
    "    \n",
    "    plt.figure(figsize=(20, 10))\n",
    "    x = line_graph_points[merged_time_column]\n",
    "    y = line_graph_points[mode.value]\n",
    "    plt.plot(x, y, label=f'Top {100 - outlier_percent*100}% {mode.value} Songs')\n",
    "    plt.xticks(x[::xticks_interval], rotation=90)\n",
    "    plt.xlabel(str(merged_time_column))\n",
    "    plt.ylabel(mode.value)\n",
    "    title = f'{mode.value} by Year and Month' if not day else f'{mode.value} by Year, Month and Day'\n",
    "    plt.title(f'{title}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # print the time which is top 20% of the points in each year and month\n",
    "    top_10_percent_points_time_period = line_graph_points[line_graph_points[mode.value] >= line_graph_points[mode.value].quantile(date_point_quantile)]\n",
    "\n",
    "    # split the year-month column into year and month\n",
    "    top_10_percent_points_time_period[group_by_columns] = top_10_percent_points_time_period[merged_time_column].str.split('-', expand=True)\n",
    "    top_10_percent_points_time_period = top_10_percent_points_time_period.drop(columns=[merged_time_column])\n",
    "    # change all the columns in group_by_columns to int\n",
    "    top_10_percent_points_time_period[group_by_columns] = top_10_percent_points_time_period[group_by_columns].astype(int)\n",
    "    # sort by year and month\n",
    "    top_10_percent_points_time_period = top_10_percent_points_time_period.sort_values(by=group_by_columns, ascending=False)\n",
    "    display(pd.DataFrame(top_10_percent_points_time_period))\n",
    "    # count the number of same month or (year) in current record\n",
    "    if not day:\n",
    "        # group the record by month and conunt the number of same month, keep month, and count only\n",
    "        top_10_percent_points_time_period = top_10_percent_points_time_period.groupby(group_by_columns[1]).size().reset_index(name='Count')\n",
    "        display(pd.DataFrame(top_10_percent_points_time_period).style.hide_index())\n",
    "        \n",
    "\n",
    "def line_graph_for_continents(data: pd.DataFrame, mode = Mode.MEAN, graph_type = GraphType.SUBPLOTS, outlier_percent: float = 0.25, n: int = -1, xticks_interval: int = 5):\n",
    "    assert 0 <= outlier_percent <= 1, 'outlier_percent must be between 0 and 1'\n",
    "    # create a data with year, month, continent and the max points of the song in each continent in each year and month\n",
    "    line_graph_region_points = data.groupby(['Year', 'Month', 'Continent', 'id'])['Points (Total)'].max()\n",
    "    line_graph_region_points = line_graph_region_points.reset_index(name='Max Points')\n",
    "    print(\"Before removing outliers: \", line_graph_region_points.shape)\n",
    "    # drop last 25% of the points in each continent in each year and month to remove outliers\n",
    "    line_graph_region_points = line_graph_region_points.sort_values(by=['Year', 'Month', 'Continent', 'Max Points'], ascending=False)\n",
    "    point_threshold = line_graph_region_points.groupby(['Year', 'Month', 'Continent'])['Max Points'].quantile(outlier_percent)\n",
    "    line_graph_region_points = line_graph_region_points.merge(point_threshold, on=['Year', 'Month', 'Continent'], suffixes=('', '_threshold'))\n",
    "    line_graph_region_points = line_graph_region_points[line_graph_region_points['Max Points'] >= line_graph_region_points['Max Points_threshold']]\n",
    "    line_graph_region_points = line_graph_region_points.drop(columns=['Max Points_threshold'])\n",
    "    line_graph_region_points = line_graph_region_points.reset_index(drop=True)\n",
    "    print(\"After removing outliers: \", line_graph_region_points.shape)\n",
    "    line_graph_region_points = line_graph_region_points.groupby(['Year', 'Month', 'Continent'])['Max Points']\n",
    "    \n",
    "    if mode == Mode.MEAN:\n",
    "        line_graph_region_points = line_graph_region_points.mean()\n",
    "    elif mode == Mode.MEDIAN:\n",
    "        line_graph_region_points = line_graph_region_points.median()\n",
    "    elif mode == Mode.SUM:\n",
    "        line_graph_region_points = line_graph_region_points.sum()\n",
    "    line_graph_region_points = line_graph_region_points.reset_index(name= mode.value)\n",
    "    line_graph_region_points['Year-Month'] = line_graph_region_points['Year'].astype(str) + '-' + line_graph_region_points['Month'].astype(str)\n",
    "    line_graph_region_points = line_graph_region_points.drop(columns=['Year', 'Month'])\n",
    "    if graph_type == GraphType.SUBPLOTS:\n",
    "        # plot the multiple line graph for each continent in each subplot\n",
    "        plt.figure(figsize=(30, 10))\n",
    "        for i, continent in enumerate(line_graph_region_points['Continent'].unique(), 1):\n",
    "            plt.subplot(3, 3, i)\n",
    "            x = line_graph_region_points[line_graph_region_points['Continent'] == continent]['Year-Month']\n",
    "            y = line_graph_region_points[line_graph_region_points['Continent'] == continent][mode.value]\n",
    "            plt.plot(x, y, label=f'Top {100 - outlier_percent*100}% {mode.value} in {continent}')\n",
    "            plt.xticks(x[::xticks_interval], rotation=90)\n",
    "            if n != -1:\n",
    "                plt.title(f'Top {n} {mode.value} Songs in {continent}')\n",
    "                top_n_songs_points = top_n_song_points(data, n)\n",
    "                top_n_songs_points = top_n_songs_points.groupby(['Year', 'Month', 'Continent'])['Max Points'].mean()\n",
    "                top_n_songs_points = top_n_songs_points.reset_index(name='Average Points')\n",
    "                top_n_songs_points['Year-Month'] = top_n_songs_points['Year'].astype(str) + '-' + top_n_songs_points['Month'].astype(str)\n",
    "                top_n_songs_points = top_n_songs_points.drop(columns=['Year', 'Month'])\n",
    "                plt.plot(top_n_songs_points[top_n_songs_points['Continent'] == continent]['Year-Month'], \n",
    "                         top_n_songs_points[top_n_songs_points['Continent'] == continent]['Average Points'], \n",
    "                         label=f'Top {n} Average Points Songs in {continent}')\n",
    "            else:\n",
    "                plt.title(f'{mode.value} by Year and Month in {continent}')\n",
    "            plt.xlabel('Year-Month')\n",
    "            plt.ylabel(mode.value)\n",
    "            plt.legend()\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    elif graph_type == GraphType.ONE_PLOT:\n",
    "        # plot another line graph for each continent in one plot\n",
    "        plt.figure(figsize=(30, 10))\n",
    "        for continent in line_graph_region_points['Continent'].unique():\n",
    "            x = line_graph_region_points[line_graph_region_points['Continent'] == continent]['Year-Month']\n",
    "            y = line_graph_region_points[line_graph_region_points['Continent'] == continent][mode.value]\n",
    "            plt.plot(x, y, label=continent)\n",
    "            plt.xticks(x[::xticks_interval], rotation=90)\n",
    "        plt.xlabel('Year-Month')\n",
    "        plt.ylabel(mode.value)\n",
    "        plt.title(f'{mode.value} by Year and Month')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "line_graph_for_continents(song_data_region, mode=Mode.MEDIAN, graph_type=GraphType.SUBPLOTS, outlier_percent=0.75, n=100, xticks_interval=3)\n",
    "line_graph_for_continents(song_data_region, mode=Mode.MEDIAN, graph_type=GraphType.ONE_PLOT, outlier_percent=0.75, n=100, xticks_interval=3)\n",
    "# line_graph_for_time_series(song_data_region, mode=Mode.MEAN, outlier_percent=0.75, xticks_interval=30, day=True)\n",
    "line_graph_for_time_series(song_data_region, mode=Mode.MEDIAN, outlier_percent=0.75, xticks_interval=1, date_point_quantile=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2612ac2",
   "metadata": {},
   "source": [
    "## Visualize the result (song)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d09c1d2",
   "metadata": {},
   "source": [
    "### Distribution of song attributes\n",
    "For loudness, it has great range of values so it should be handled by normalization. For instrumentalness, it has a lot of 0 values and does not contain much information. For Danceability and Energy, Speechiness and Acousticness, they have similar shape of distribution and they could be correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eb7531",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_attributes = ['Danceability', 'Energy', 'Loudness', 'Speechiness', 'Acousticness', 'Instrumentalness', 'Valence']\n",
    "\n",
    "def plot_distribution(data: pd.DataFrame, columns: list = song_attributes):\n",
    "\n",
    "    # plot the distribution of each song attribute in subplots\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    for i, attribute in enumerate(columns, 1):\n",
    "        plt.subplot(3, 3, i)\n",
    "        sns.histplot(data=data, x=attribute, kde=True)\n",
    "        plt.title(f'Distribution of {attribute}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_correlation(data: pd.DataFrame):\n",
    "    # plot the correlation matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(data.corr(), annot=True, cmap='viridis')\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def normalize(data: pd.DataFrame, columns: list = song_attributes):\n",
    "    # normalize the data\n",
    "    scaler = MinMaxScaler()\n",
    "    data[columns] = scaler.fit_transform(data[columns])\n",
    "    return data\n",
    "\n",
    "def remove_outliers(data: pd.DataFrame, columns: list = song_attributes):\n",
    "    model = IsolationForest(n_estimators=100, max_samples='auto', contamination=float(0.05), max_features=1.0, bootstrap=False, n_jobs=-1, random_state=42, verbose=1)\n",
    "    model.fit(data[columns])\n",
    "    data['anomaly'] = pd.Series(model.predict(data[columns]))\n",
    "    data = data[data['anomaly'] == 1]\n",
    "    data = data.drop(columns=['anomaly'])\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    return data\n",
    "    \n",
    "    \n",
    "normalized_data = normalize(song_data_only, song_attributes)\n",
    "print(\"Before removing outliers: \", normalized_data.shape)\n",
    "cleaned_song_data = remove_outliers(normalized_data, song_attributes)\n",
    "print(\"After removing outliers: \", cleaned_song_data.shape)\n",
    "plot_distribution(cleaned_song_data, song_attributes)\n",
    "plot_correlation(cleaned_song_data)\n",
    "\n",
    "refined_song_attributes = ['Danceability', 'Energy', 'Loudness', 'Speechiness', 'Acousticness', 'Valence']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d7106a",
   "metadata": {},
   "source": [
    "### PCA dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0944451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca\n",
    "def pca(data: pd.DataFrame, target_columns: list = song_attributes, n_components: int = 2):\n",
    "    pca = PCA(n_components=n_components)\n",
    "\n",
    "    pca_data = pca.fit_transform(data[target_columns])\n",
    "\n",
    "    pca_data = pd.DataFrame(pca_data, columns=[f'PC{i}' for i in range(1, n_components+1)])\n",
    "    return pca_data\n",
    "    \n",
    "def get_knee_n(data: pd.DataFrame, target_columns: list = song_attributes):\n",
    "    number_of_components = len(target_columns)\n",
    "    pca = PCA(n_components=number_of_components)\n",
    "    pca.fit_transform(data[target_columns])\n",
    "    cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "    # plot the cumulative variance ratio\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, len(cumulative_variance_ratio)+1), cumulative_variance_ratio)\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "    plt.title('Cumulative Explained Variance Ratio vs Number of Components')\n",
    "    plt.show()\n",
    "    \n",
    "    kn = KneeLocator(range(1, len(cumulative_variance_ratio)+1), cumulative_variance_ratio, curve='concave', direction='increasing')\n",
    "    print(f'Knee Point: {kn.knee}')\n",
    "    print(f'Explained Variance Ratio: {cumulative_variance_ratio[kn.knee-1]}')\n",
    "    return kn.knee\n",
    "\n",
    "def plot_pca_graph(pca_data: pd.DataFrame, data: pd.DataFrame, label_column: str = 'Continent', n_components: int = 2):\n",
    "    # plot 3d scatter plot\n",
    "    if n_components == 3:\n",
    "        fig = plt.figure(figsize=(8, 6))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        # map the continent into colors\n",
    "        color_map = ListedColormap(plt.cm.Set1.colors[:len(data[label_column].unique())])\n",
    "        ax.scatter(pca_data['PC1'], pca_data['PC2'], pca_data['PC3'], c=data[label_column].map({continent: i for i, continent in enumerate(data[label_column].unique())}), cmap=color_map)\n",
    "        ax.set_xlabel('PC1')\n",
    "        ax.set_ylabel('PC2')\n",
    "        ax.set_zlabel('PC3')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "n = get_knee_n(cleaned_song_data, refined_song_attributes)\n",
    "pca_data = pca(cleaned_song_data, refined_song_attributes, n_components=n)\n",
    "plot_pca_graph(pca_data, cleaned_song_data, label_column='Continent', n_components=n)\n",
    "pca_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0185d3a9",
   "metadata": {},
   "source": [
    "## Song genre classification\n",
    "1. Label them with clusters (Failed)\n",
    "2. Train models for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9655c73c",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "KMEANS:\n",
    "Number of Clusters: 3\n",
    "Silhouette Score: 0.3275839637360091\n",
    "Davies Bouldin Score: 1.0621302261389547\n",
    "\n",
    "DBSCAN:\n",
    "Best Silhouette Score:  0.38574756612553013  Best Davies Bouldin Score:  0.5332216963398735\n",
    "eps:  0.15  min_samples:  11\n",
    "Number of Clusters:  2\n",
    "\n",
    "Number of Clusters:  2\n",
    "Silhouette Score: 0.30942188038331087\n",
    "Davies Bouldin Score: 1.3464118389346242\n",
    "\n",
    "As a result, the song attributes are not good enough to cluster the songs into different genres. The silhouette score is not high enough to cluster the songs into different genres. The reason could be that there are many overlapping in different sets of songs. For example, the songs in the genre of pop could be similar to the songs in the genre of rock. So it is hard to cluster the songs into different genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352cedc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_clustering(pca_data: pd.DataFrame, data: pd.DataFrame):\n",
    "    wcss = []\n",
    "    for i in range(1, 11):\n",
    "        kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)\n",
    "        kmeans.fit(pca_data)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "    # plot the elbow method\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, 11), wcss)\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('WCSS')\n",
    "    \n",
    "    kn = KneeLocator(range(1, 11), wcss, curve='convex', direction='decreasing')\n",
    "    kmeans = KMeans(n_clusters=kn.knee, init='k-means++', random_state=42)\n",
    "    print(f'Number of Clusters: {kn.knee}')\n",
    "    kmeans.fit(pca_data)\n",
    "    \n",
    "    silhouette_avg = silhouette_score(pca_data, kmeans.labels_)\n",
    "    print(f'Silhouette Score: {silhouette_avg}')\n",
    "    davies_bouldin = davies_bouldin_score(pca_data, kmeans.labels_)\n",
    "    print(f'Davies Bouldin Score: {davies_bouldin}')\n",
    "    \n",
    "    # add the cluster column to the data\n",
    "    new_data = data.copy()\n",
    "    new_data['Genre'] = kmeans.labels_\n",
    "    \n",
    "    \n",
    "    return new_data\n",
    "\n",
    "def dbscan_clustering(pca_data: pd.DataFrame, data: pd.DataFrame):\n",
    "    eps_values = [0.13, 0.14, 0.15, 0.16, 0.17]\n",
    "    min_samples_values = [11, 12, 13]\n",
    "    silhouette_avg_values = []\n",
    "    davies_bouldin_values = []\n",
    "    cluster_numbers = []\n",
    "    selected_eps_val = 0\n",
    "    selected_sample_val = 0\n",
    "    max_silhouette_avg = 0\n",
    "    max_davies_bouldin = 0\n",
    "    for eps in eps_values:\n",
    "        for min_samples in min_samples_values:\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "            clusters = dbscan.fit_predict(pca_data)\n",
    "            if len(set(clusters)) == 1:\n",
    "                silhouette_avg_values.append(-1)\n",
    "                davies_bouldin_values.append(-1)\n",
    "                cluster_numbers.append(1)\n",
    "            else:\n",
    "                silhouette_avg_values.append(silhouette_score(pca_data, clusters))\n",
    "                \n",
    "                davies_bouldin_values.append(davies_bouldin_score(pca_data, clusters))\n",
    "                cluster_numbers.append(len(set(clusters)))\n",
    "                if len(silhouette_avg_values) == 1:\n",
    "                    selected_eps_val = eps\n",
    "                    selected_sample_val = min_samples\n",
    "                    max_silhouette_avg = silhouette_avg_values[-1]\n",
    "                    max_davies_bouldin = davies_bouldin_values[-1]\n",
    "                elif silhouette_avg_values[-1] > max_silhouette_avg and davies_bouldin_values[-1] < max_davies_bouldin:\n",
    "                    max_silhouette_avg = silhouette_avg_values[-1]\n",
    "                    max_davies_bouldin = davies_bouldin_values[-1]\n",
    "                    selected_eps_val = eps\n",
    "                    selected_sample_val = min_samples\n",
    "    \n",
    "    # plot the silhouette score\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(len(silhouette_avg_values)), silhouette_avg_values)\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.show()\n",
    "    \n",
    "    selected_index = np.argmax(silhouette_avg_values)\n",
    "    dbscan = DBSCAN(eps=selected_eps_val, min_samples=selected_sample_val)\n",
    "    \n",
    "    print(\"Best Silhouette Score: \", max_silhouette_avg, \" Best Davies Bouldin Score: \", max_davies_bouldin)\n",
    "    print('eps: ', selected_eps_val, ' min_samples: ', selected_sample_val)\n",
    "    print(\"Number of Clusters: \", cluster_numbers[selected_index])\n",
    "    \n",
    "    new_data = data.copy()\n",
    "    new_data['Genre'] = dbscan.fit_predict(pca_data)\n",
    "    return new_data\n",
    "\n",
    "def hierarchical_clustering(pca_data: pd.DataFrame, data: pd.DataFrame):\n",
    "    hierarchical = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')\n",
    "    labels = hierarchical.fit_predict(pca_data)\n",
    "    \n",
    "    print(\"Number of Clusters: \", len(set(labels)))\n",
    "    silhouette_avg = silhouette_score(pca_data, labels)\n",
    "    print(f'Silhouette Score: {silhouette_avg}')\n",
    "    davies_bouldin = davies_bouldin_score(pca_data, labels)\n",
    "    print(f'Davies Bouldin Score: {davies_bouldin}')\n",
    "    \n",
    "    new_data = data.copy()\n",
    "    new_data['Genre'] = labels\n",
    "    \n",
    "    return new_data\n",
    "    \n",
    "# song_data_with_genre = dbscan_clustering(pca_data, cleaned_song_data)\n",
    "# song_data_with_genre = kmeans_clustering(pca_data, cleaned_song_data)\n",
    "song_data_with_genre = hierarchical_clustering(pca_data, cleaned_song_data)\n",
    "# song_data_with_genre.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aede344b",
   "metadata": {},
   "source": [
    "#### Time series analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb0a0ce",
   "metadata": {},
   "source": [
    "##### Visualize the number of points of different months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cf5554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution graph for each year and month versus score\n",
    "def plot_distribution_graph_for_year_month(song_data_region: pd.DataFrame, year: int, month: int):\n",
    "    data = song_data_region.groupby(['Year', 'Month', 'Continent', 'id'])['Points (Total)'].mean()\n",
    "    data = data.reset_index(name='Average Points')\n",
    "    mean = data[(data['Year'] == year) & (data['Month'] == month)]['Average Points'].mean()\n",
    "    median = data[(data['Year'] == year) & (data['Month'] == month)]['Average Points'].median()\n",
    "    std = data[(data['Year'] == year) & (data['Month'] == month)]['Average Points'].std()\n",
    "    q1 = data[(data['Year'] == year) & (data['Month'] == month)]['Average Points'].quantile(0.25)\n",
    "    q3 = data[(data['Year'] == year) & (data['Month'] == month)]['Average Points'].quantile(0.75)\n",
    "    \n",
    "    plt.figure(figsize=(20, 10))\n",
    "    sns.displot(data[(data['Year'] == year) & (data['Month'] == month)]['Average Points'], kde=True)\n",
    "    plt.xlim(0, 200)\n",
    "    plt.xlabel('Average Points')\n",
    "    plt.ylabel('Number of Songs')\n",
    "    plt.title(f'Distribution of Average Points in {year}-{month}')\n",
    "    # legend the mean, median and standard deviation\n",
    "    plt.axvline(mean, color='red', label='Mean')\n",
    "    plt.axvline(median, color='green', label='Median')\n",
    "    plt.axvline(q1, color='purple', label='Q1')\n",
    "    plt.axvline(q3, color='purple', label='Q3')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    display(pd.DataFrame({'Mean': [mean], 'Median': [median], 'Standard Deviation': [std], 'Q1': [q1], 'Q3': [q3]}))\n",
    "\n",
    "def plot_distribution_graph_for_year(song_data_region: pd.DataFrame, year: int):\n",
    "    sns.set()\n",
    "    data = song_data_region.groupby(['Year', 'Month', 'Continent', 'id'])['Points (Total)'].mean()\n",
    "    data = data.reset_index(name='Average Points')\n",
    "\n",
    "    fig, axes = plt.subplots(4, 3, figsize=(30, 30))\n",
    "    for i, month in enumerate(range(1, 13), 1):\n",
    "        sub_data = data[(data['Year'] == year) & (data['Month'] == month)]['Average Points']\n",
    "        sns.histplot(sub_data, kde=True, ax=axes[(i-1)//3, (i-1)%3])\n",
    "        axes[(i-1)//3, (i-1)%3].set_xlim(0, 200)\n",
    "        axes[(i-1)//3, (i-1)%3].set_xlabel('Average Points')\n",
    "        axes[(i-1)//3, (i-1)%3].set_ylabel('Number of Songs')\n",
    "        axes[(i-1)//3, (i-1)%3].set_title(f'Distribution of Average Points in {year}-{month}')\n",
    "    plt.show()\n",
    "    \n",
    "plot_distribution_graph_for_year(song_data_region, 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f2a958",
   "metadata": {},
   "source": [
    "对歌曲去重，在数据集中，大部分音乐由于有多个Artist导致重复出现，我们把有多个Artist的歌曲（即重复歌曲）去除\n",
    "因为数据集是每日的排行榜数据，所以许多歌曲会重新上榜，我们根据去除掉重复歌曲的全部的数据进行处理，取每一首歌的最大值/平均值进行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b33f89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设 'df' 是您的DataFrame名称\n",
    "# 1. 将 'Date' 列转换为 datetime 对象，以确保正确处理\n",
    "spotify_song_data['Date'] = pd.to_datetime(spotify_song_data['Date'], format='%d/%m/%Y')  # 调整 format 以匹配您的实际日期格式\n",
    "\n",
    "# 2. 删除具有相同 'Title' 和 'Date' 但 'Artists' 不同的重复项\n",
    "df_unique = spotify_song_data.drop_duplicates(subset=['Title', 'Date'])\n",
    "\n",
    "# 3. 如果需要，可以重置索引\n",
    "df_unique.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# 显示更新后的DataFrame\n",
    "df_unique.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5599cc27",
   "metadata": {},
   "source": [
    "当前数据Date列可以看出，是2017-2023年中每日的Top200歌曲排行榜，但是数据量过大，观测发现每月的歌曲平均分数变化不大，所以我们按每月的平均分作为歌曲的平均分表示，Date数据的形式变为年-月，其他数据仍然保留"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3f03d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_song_data['Date'] = pd.to_datetime(spotify_song_data['Date'], format='%d/%m/%Y')\n",
    "\n",
    "spotify_song_data['Data_Month'] = spotify_song_data['Date'].dt.to_period('M')\n",
    "\n",
    "spotify_song_data.head()\n",
    "\n",
    "average_monthly_points = spotify_song_data.groupby(['id', 'Data_Month'])['Points (Total)'].mean().reset_index()\n",
    "\n",
    "average_monthly_points.rename(columns={'Points (Total)': 'Average_Points'}, inplace=True)\n",
    "\n",
    "monthly_data = pd.merge(spotify_song_data, average_monthly_points, on=['id', 'Data_Month'], how='left')\n",
    "\n",
    "columns_to_drop = ['Points (Total)', 'Points (Ind for each Artist/Nat)', 'Date', 'Rank']\n",
    "monthly_data.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "monthly_data.drop_duplicates(inplace=True)\n",
    "\n",
    "monthly_data.head(500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ab2394",
   "metadata": {},
   "outputs": [],
   "source": [
    "ella_data = monthly_data[monthly_data['Title'] == 'Ella Baila Sola']\n",
    "ella_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c270e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42e91230",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "440b1bf0",
   "metadata": {},
   "source": [
    "缩小数据样本量：我们发现每月的排行榜中，大部分数据的平均Point变化不大，所以根据Date取每月平均分作为歌曲的Point，并且新增一个Data_Month列，数据集不再是每日的歌曲排行榜，而应该是每月的数据排行榜，并且Point是音乐的平均值"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
